+++
categories = []
date = 2021-07-12T16:00:00Z
tags = ["Elasticsearch"]
title = "Elasticsearch分片副本那些事"
url = "/post/elasticsearch-shard-replica"

+++
### 前言

从某种角度上说，ES是十分易用的，利用ES完成一些基本的写入和查询操作，只需要简单看下文档就能学会。但是ES又有很多配置可以自定义，当你有实际的业务需要时，如何能有效利用现有的资源达到比较好的时延和吞吐量确又非常困难。本文就聊一下分片（shard）和副本（replica），来看看究竟应该如何选择一个合适的分片和副本数。

### 分片副本是什么

分片类似于数据库的分库分表，将一个索引里的数据分到不同的分片中。在写入过程中，通过相应的路由手段（默认规则是分片编号=hash(_id)%总分片数）写入相应的分片。在查询过程中，会分别查询所有分片并将结果汇总得到最终查询结果，这样就可以将非常大量的索引数据分散到不同的分片中，由于每个分片的查询都使用一个线程，这样可以有效地减小单次查询的时延。

副本其实很容易理解，用处就是保证当部分节点掉出集群时保证ES集群的可用性。副本数量越多，能容忍节点脱离的数量就越多。另外，副本也是一种分片，也可以执行查询任务。

### 分片副本数量应该怎么选

副本数量比较好选，根据你需要的可用性选择就好。另外，副本越多，写入消耗就越大（相当于写了不止一份的数据）。

接下来，我们主要聊一下查询过程中一个节点上的分片数量应该有几个（这里说的分片就包含了副本，因为副本也会参与到查询中去）。

既然分片数量增加，单个线程可以更快地完成单shard的查询，那么是不是分片越多越好呢？其实不然，分片数量过多会导致以下三个问题：

1. 多个shard并发查询会使用到更多的线程数，这样会增大CPU上下文切换次数，可能会增大时延
2. 一次查询会查询多个shard，并将结果合并，这会受木桶效应影响，一旦某一个或几个shard的查询时延增大，总的查询时延也会受到影响。（在这种情况下，网络波动是一个容易出现的影响因素）
3. ES使用主从机制，shard信息的元数据需要master节点管理。当shard数量增多时，master节点同步元数据的压力会增大，可能会影响集群的可用性。ES在7版本之后增加了一个参数cluster.max_shards_per_node限制单节点shard数量不超过1000

分片最少一个节点一个（shard数量为1，replica数量为node数-1），这种情况下由于无法利用分片并发查询，时延会比较高。那么最多应该是多少呢？

其实这里可以参考ES源码中SEARCH线程池的设置，保持单节点同时被查询的分片数不超过SEARCH线程池的大小（即1.5倍核数+1）。因为如果超过这个数值的话，同一时刻的查询任务就肯定会遇到排队的情况。排队就会影响到查询的时延和吞吐量。这是为什么呢？

### 吞吐量和时延

首先，时延这个问题很好理解，可以参考下图：

![](/images/no-split.png)

假设有一次查询需要查询10个分片，但是线程池只有6个线程可用。这个时候有2个线程在后2秒的时间里无事可做，时延必定会收到影响。这里可以继续延伸一下得到一个很有趣的结论，并行的任务数如果能够整除线程数，那么时延是不会受到影响的。还有一点需要注意，如果一次查询没有跑满CPU，时延也是会延长的。

接着再说一下吞吐量的问题。事实上，在CPU占用100%的理想情况下（无IO），吞吐量不会收到分片数量的影响。举个例子，假设CPU核数是4，单节点有1个分片和单节点有4个分片的情况下，单位时间内响应的请求数量是一样多的。（比如跑慢满4个核，1个分片单位时间可以响应4个请求，4个分片单位时间内也可以响应四个请求）。但是由于有了IO的损耗，并行多任务的IO时间消耗不等于原时间/任务数。换句话说：将大任务分解成小任务的CPU耗时不变，但会增加IO的耗时，从而影响吞吐量。

### SEARCH线程池为什么是1.5倍核数+1

这里我可以举一个形象的例子，假设CPU核数是高速路的车道数C，每一条车道对应一个核，收费站数量则是线程数P。现在假设P=C=10，并且每一辆车都用ETC过收费站（假设占了10个CPU时间片），这种情况下每一个任务（车）都正好占了一个核（收费站），此时CPU被占满。假设所有ETC换成了人工收费口，过收费口时有一半的时间都被浪费在了掏手机扫码上（5个时间片IO，5个时间片占用CPU），这种情况下进去10辆车，只能出5辆车，CPU占用率只有一半。此时如果想要占满CPU，那么就需要提高线程数，让一个收费员负责两个车道，这样分下来就可以进去10辆车，出来10辆车了。

通过这个例子，可以推断出：ES假设了平均情况下一个查询时间中CPU时间:IO时间=2:1，因为这样设置SEARCH线程可以将CPU跑满。这样我们可以得出一个结论：单节点最多被查询的分片数量为1.5倍核数+1

那么单节点同时被查询分片数量的上限可以认为是1.5倍核数+1，因为这种情况下CPU会被占满。从上面对于吞吐量和时延的分析来看，这样子会达到查询收益的最大化。

### 总结

当然，本文给出的结论仅仅是启发式的，上面这种计算方式只能作为一种预估的手段，毕竟做了许多的理想化假设。并且实际情况下，集群会有写入等其他操作，不可能让查询占满CPU资源。另外，如果集群出现节点脱离集群的问题，分片容量过大会造成集群无法很快恢复，这里ES官方给出的建议是单分片不要超过50GB大小。

所以，综上所述，分片数量的选择仍然是很复杂，需要根据实际情况做出调整。

### 参考链接

1. [如何估算吞吐量以及线程池大小](https://chanjarster.github.io/post/concurrent-programming/throughput-and-thread-pool-size/)
2. [并行、延迟与吞吐量](https://chanjarster.github.io/post/concurrent-programming/parallel-latency-throughput/)
3. [聊聊 Elasticsearch 的查询毛刺](https://www.easyice.cn/archives/361)
4. [How many shards should I have in my Elasticsearch cluster?](https://www.elastic.co/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster)