<!doctype html><html lang=en><head><meta charset=utf-8><meta name=generator content="Hugo 0.82.0"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Fan Jingbo"><meta property="og:url" content="https://fanjingbo.com/post/center_loss_pytorch/"><link rel=canonical href=https://fanjingbo.com/post/center_loss_pytorch/><link rel=preload href=/js/highlight.pack.js as=script><link rel=alternate type=application/atom+xml href=https://fanjingbo.comindex.xml title="Fan Jingbo's Blog"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/fanjingbo.com"},"articleSection":"post","name":"Center Loss的Pytorch实现","headline":"Center Loss的Pytorch实现","description":"Center Loss是2016年ECCV的一篇人脸识别文章中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的\n一、Center Loss的原理 要实现Center Loss，必须知道Center Loss的原理。\nCenter Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。\nCenter Loss的流程大致如下：\n  保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param  前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)\n  反向传播时，feature的梯度公式如下： 中心值的梯度是由作者定义的，公式如下：\n  这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。\n 注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）\n 二、Pytorch中的backward 那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的backward方法\n我们看一下官方文档中backward的基本用法\n从文档可以看出：\n 当variables是标量时，不用指定grad_variables（事实上，此时grad_variables为1），这种情况就是一般的loss.backward()这种用法。 当variables为非标量且require_grad为True时，需要指定grad_variables，文档中对grad_variables的解释为“gradient of the differentiated function w.r.t. corresponding variables”，其实意思就是损失函数相对与variables的梯度  因此backward可以实现两种情况，一种是傻瓜式的，给一个loss，backward可以把所有前层require_grad=True的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道grad_variables了。原理和链式法则一模一样。\n三、实现Center Loss 到现在，解决方案已经呼之欲出了：进行两次backward即可。如图：\nCenter Loss层有两个变量，进行loss.backward()，两个变量都会求出对应的梯度。而centers_param.backward(man_set_centers_grad)，则可以直接把man_set_centers_grad赋值给variable的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个backward，就可以实现想要的手动更新。不过需要注意的是，连续两次backward，会把两次梯度累加。所以在第一次backward后使用zero_grad()方法，把梯度置零即可。\n 我的代码：https:\/\/github.com\/flyingpot\/center_loss_pytorch\n ","inLanguage":"en-US","author":"Fan Jingbo","creator":"Fan Jingbo","publisher":"Fan Jingbo","accountablePerson":"Fan Jingbo","copyrightHolder":"Fan Jingbo","copyrightYear":"2018","datePublished":"2018-03-16 08:07:05 \u002b0800 CST","dateModified":"2018-03-16 08:07:05 \u002b0800 CST","url":"https:\/\/fanjingbo.com\/post\/center_loss_pytorch\/","keywords":[]}</script><title>Center Loss的Pytorch实现 - Fan Jingbo's Blog</title><meta property="og:title" content="Center Loss的Pytorch实现 - Fan Jingbo's Blog"><meta property="og:type" content="article"><meta property="og:description" content="Center Loss是2016年ECCV的一篇人脸识别文章中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的
一、Center Loss的原理 要实现Center Loss，必须知道Center Loss的原理。
Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。
Center Loss的流程大致如下：
  保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param  前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)
  反向传播时，feature的梯度公式如下： 中心值的梯度是由作者定义的，公式如下：
  这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。
 注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）
 二、Pytorch中的backward 那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的backward方法
我们看一下官方文档中backward的基本用法
从文档可以看出：
 当variables是标量时，不用指定grad_variables（事实上，此时grad_variables为1），这种情况就是一般的loss.backward()这种用法。 当variables为非标量且require_grad为True时，需要指定grad_variables，文档中对grad_variables的解释为“gradient of the differentiated function w.r.t. corresponding variables”，其实意思就是损失函数相对与variables的梯度  因此backward可以实现两种情况，一种是傻瓜式的，给一个loss，backward可以把所有前层require_grad=True的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道grad_variables了。原理和链式法则一模一样。
三、实现Center Loss 到现在，解决方案已经呼之欲出了：进行两次backward即可。如图：
Center Loss层有两个变量，进行loss.backward()，两个变量都会求出对应的梯度。而centers_param.backward(man_set_centers_grad)，则可以直接把man_set_centers_grad赋值给variable的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个backward，就可以实现想要的手动更新。不过需要注意的是，连续两次backward，会把两次梯度累加。所以在第一次backward后使用zero_grad()方法，把梯度置零即可。
 我的代码：https://github.com/flyingpot/center_loss_pytorch
 "><meta name=description content="Center Loss是2016年ECCV的一篇人脸识别文章中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的
一、Center Loss的原理 要实现Center Loss，必须知道Center Loss的原理。
Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。
Center Loss的流程大致如下：
  保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param  前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)
  反向传播时，feature的梯度公式如下： 中心值的梯度是由作者定义的，公式如下：
  这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。
 注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）
 二、Pytorch中的backward 那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的backward方法
我们看一下官方文档中backward的基本用法
从文档可以看出：
 当variables是标量时，不用指定grad_variables（事实上，此时grad_variables为1），这种情况就是一般的loss.backward()这种用法。 当variables为非标量且require_grad为True时，需要指定grad_variables，文档中对grad_variables的解释为“gradient of the differentiated function w.r.t. corresponding variables”，其实意思就是损失函数相对与variables的梯度  因此backward可以实现两种情况，一种是傻瓜式的，给一个loss，backward可以把所有前层require_grad=True的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道grad_variables了。原理和链式法则一模一样。
三、实现Center Loss 到现在，解决方案已经呼之欲出了：进行两次backward即可。如图：
Center Loss层有两个变量，进行loss.backward()，两个变量都会求出对应的梯度。而centers_param.backward(man_set_centers_grad)，则可以直接把man_set_centers_grad赋值给variable的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个backward，就可以实现想要的手动更新。不过需要注意的是，连续两次backward，会把两次梯度累加。所以在第一次backward后使用zero_grad()方法，把梯度置零即可。
 我的代码：https://github.com/flyingpot/center_loss_pytorch
 "><meta property="og:locale" content="en-us"><link rel=stylesheet href=/css/flexboxgrid-6.3.1.min.css><link rel=stylesheet href=/css/github-markdown.css><link rel=stylesheet href=/css/highlight/tomorrow.min.css><link rel=stylesheet href=/css/index.css><link href=/index.xml rel=alternate type=application/rss+xml title="Fan Jingbo's Blog"><link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker|Bree+Serif&display=swap" rel=stylesheet></head><body><article class="post Chinese" id=article><div class=row><div class=col-xs-12><div class=site-header><header><div class="signatures site-title"><a href=/>Fan Jingbo</a></div></header><div class="row end-xs"></div><div class=header-line></div></div><header class=post-header><h1 class=post-title>Center Loss的Pytorch实现</h1><div class="row post-desc"><div class=col-xs-6><time class=post-date datetime="2018-03-16 08:07:05 CST">16 Mar 2018</time></div><div class=col-xs-6><div class=post-author><a target=_blank href=https://fanjingbo.com>@Fan Jingbo</a></div></div></div></header><div class="post-content markdown-body"><p>Center Loss是2016年ECCV的一篇人脸识别<a href=https://ydwen.github.io/papers/WenECCV16.pdf>文章</a>中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的</p><h2 id=一center-loss的原理>一、Center Loss的原理</h2><p>要实现Center Loss，必须知道Center Loss的原理。</p><p>Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。</p><p><img src=/images/center_loss.jpeg alt=center\_loss>
Center Loss的流程大致如下：</p><ol><li><p>保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param</p></li><li><p>前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)</p></li><li><p>反向传播时，feature的梯度公式如下：
<img src=/images/derivative.png alt=derivative>
中心值的梯度是由作者定义的，公式如下：</p></li></ol><p><img src=/images/center_loss_formula.png alt=center\_loss\_formula>
这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。</p><blockquote><p>注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）</p></blockquote><h2 id=二pytorch中的backward>二、Pytorch中的backward</h2><p>那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的<code>backward</code>方法</p><p>我们看一下官方文档中<code>backward</code>的基本用法</p><p><img src=/images/torch_autograd_backward.png alt=torch\_autograd\_backward>
从文档可以看出：</p><ol><li>当<code>variables</code>是标量时，不用指定<code>grad_variables</code>（事实上，此时<code>grad_variables</code>为1），这种情况就是一般的<code>loss.backward()</code>这种用法。</li><li>当variables为非标量且<code>require_grad</code>为<code>True</code>时，需要指定<code>grad_variables</code>，文档中对<code>grad_variables</code>的解释为<code>“gradient of the differentiated function w.r.t. corresponding variables”</code>，其实意思就是损失函数相对与<code>variables</code>的梯度</li></ol><p>因此<code>backward</code>可以实现两种情况，一种是傻瓜式的，给一个<code>loss</code>，<code>backward</code>可以把所有前层<code>require_grad=True</code>的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道<code>grad_variables</code>了。原理和链式法则一模一样。</p><p><img src=/images/back_propagation.png alt=back\_propagation></p><h2 id=三实现center-loss>三、实现Center Loss</h2><p>到现在，解决方案已经呼之欲出了：进行两次<code>backward</code>即可。如图：</p><p><img src=/images/backward.png alt=backward>
Center Loss层有两个变量，进行<code>loss.backward()</code>，两个变量都会求出对应的梯度。而<code>centers_param.backward(man_set_centers_grad)</code>，则可以直接把<code>man_set_centers_grad</code>赋值给<code>variable</code>的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个<code>backward</code>，就可以实现想要的手动更新。不过需要注意的是，连续两次<code>backward</code>，会把两次梯度累加。所以在第一次<code>backward</code>后使用<code>zero_grad()</code>方法，把梯度置零即可。</p><blockquote><p>我的代码：<a href=https://github.com/flyingpot/center_loss_pytorch>https://github.com/flyingpot/center_loss_pytorch</a></p></blockquote></div><div class="row middle-xs"><div class=col-xs-12></div></div><div class=row><div class=col-xs-12></div></div><div style=height:50px></div><div class=site-footer><div class=site-footer-item><a href=http://beian.miit.gov.cn target=_blank>粤ICP备18020202号-1</a></div><div class=site-footer-item><a href=https://github.com/flyingpot target=_blank>Github</a></div></div></div></div></article><script src=/js/highlight.pack.js></script><script>hljs.initHighlightingOnLoad()</script></body></html>