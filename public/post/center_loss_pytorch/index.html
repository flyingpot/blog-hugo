<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.53" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="Fanjingbo" />
  <meta property="og:url" content="https://fanjingbo.com/post/center_loss_pytorch/" />
  <link rel="canonical" href="https://fanjingbo.com/post/center_loss_pytorch/" />
  <link rel="shortcut icon" href="https://fanjingbo.com/logo.png" type="image/x-png" />

  <script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https://fanjingbo.com"
      },
      "articleSection" : "post",
      "name" : "Center Loss的Pytorch实现",
      "headline" : "Center Loss的Pytorch实现",
      "description" : " Center Loss是2016年ECCV的一篇人脸识别文章中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的
一、Center Loss的原理 要实现Center Loss，必须知道Center Loss的原理。
Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。
Center Loss的流程大致如下：
 保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param
 前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)
 反向传播时，feature的梯度公式如下： 中心值的梯度是由作者定义的，公式如下：
  这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。
 注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）
 二、Pytorch中的backward 那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的backward方法
我们看一下官方文档中backward的基本用法
从文档可以看出：
 当variables是标量时，不用指定grad_variables（事实上，此时grad_variables为1），这种情况就是一般的loss.backward()这种用法。
 当variables为非标量且require_grad为True时，需要指定grad_variables，文档中对grad_variables的解释为“gradient of the differentiated function w.r.t. corresponding variables”，其实意思就是损失函数相对与variables的梯度  因此backward可以实现两种情况，一种是傻瓜式的，给一个loss，backward可以把所有前层require_grad=True的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道grad_variables了。原理和链式法则一模一样。
三、实现Center Loss 到现在，解决方案已经呼之欲出了：进行两次backward即可。如图：
Center Loss层有两个变量，进行loss.backward()，两个变量都会求出对应的梯度。而centers_param.backward(man_set_centers_grad)，则可以直接把man_set_centers_grad赋值给variable的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个backward，就可以实现想要的手动更新。不过需要注意的是，连续两次backward，会把两次梯度累加。所以在第一次backward后使用zero_grad()方法，把梯度置零即可。
 我的代码：https://github.com/flyingpot/center_loss_pytorch
 ",
      "inLanguage" : "en-US",
      "author" : "Fanjingbo",
      "creator" : "Fanjingbo",
      "publisher": "Fanjingbo",
      "accountablePerson" : "Fanjingbo",
      "copyrightHolder" : "Fanjingbo",
      "copyrightYear" : "2018",
      "datePublished": "2018-03-16 08:07:05 &#43;0800 CST",
      "dateModified" : "2018-03-16 08:07:05 &#43;0800 CST",
      "url" : "https://fanjingbo.com/post/center_loss_pytorch/",
      "keywords" : [  ]
  }
</script>
<title>Center Loss的Pytorch实现 - Fan Jingbo&#39;s Blog</title>
  <meta property="og:title" content="Center Loss的Pytorch实现 - Fan Jingbo&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta name="description" content=" Center Loss是2016年ECCV的一篇人脸识别文章中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的
一、Center Loss的原理 要实现Center Loss，必须知道Center Loss的原理。
Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。
Center Loss的流程大致如下：
 保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param
 前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)
 反向传播时，feature的梯度公式如下： 中心值的梯度是由作者定义的，公式如下：
  这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。
 注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）
 二、Pytorch中的backward 那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的backward方法
我们看一下官方文档中backward的基本用法
从文档可以看出：
 当variables是标量时，不用指定grad_variables（事实上，此时grad_variables为1），这种情况就是一般的loss.backward()这种用法。
 当variables为非标量且require_grad为True时，需要指定grad_variables，文档中对grad_variables的解释为“gradient of the differentiated function w.r.t. corresponding variables”，其实意思就是损失函数相对与variables的梯度  因此backward可以实现两种情况，一种是傻瓜式的，给一个loss，backward可以把所有前层require_grad=True的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道grad_variables了。原理和链式法则一模一样。
三、实现Center Loss 到现在，解决方案已经呼之欲出了：进行两次backward即可。如图：
Center Loss层有两个变量，进行loss.backward()，两个变量都会求出对应的梯度。而centers_param.backward(man_set_centers_grad)，则可以直接把man_set_centers_grad赋值给variable的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个backward，就可以实现想要的手动更新。不过需要注意的是，连续两次backward，会把两次梯度累加。所以在第一次backward后使用zero_grad()方法，把梯度置零即可。
 我的代码：https://github.com/flyingpot/center_loss_pytorch
 " />

  <link
    rel="stylesheet"
    href="https://lib.baomitu.com/flexboxgrid/6.3.1/flexboxgrid.min.css"
  />
  <link
    rel="stylesheet"
    href="https://lib.baomitu.com/github-markdown-css/2.10.0/github-markdown.min.css"
  />
  <link
    rel="stylesheet"
    href="https://lib.baomitu.com/highlight.js/9.13.1/styles/tomorrow.min.css"
  />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Fan Jingbo&#39;s Blog">
  
  <script>
    

    (function(undefined) {}).call('object' === typeof window && window || 'object' === typeof self && self || 'object' === typeof global && global || {});
  </script>

  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-132076423-1"></script><script src="https://fanjingbo.com/analytics.js"></script>
</head>


  <body>
    <article class="post Chinese" id="article">
      <div class="row">
        <div class="col-xs-12 col-md-8 col-md-offset-2 col-lg-6 col-lg-offset-3">
          <a href="/">
            <div class="head-line"></div>
          </a>
          <header class="post-header">
            <h1 class="post-title">Center Loss的Pytorch实现</h1>
            <div class="row">
              <div class="col-xs-6">
                <time class="post-date" datetime="2018-03-16 08:07:05 CST">
                  16 Mar 2018
                </time>
              </div>
              <div class="col-xs-6">
                <div class="post-author">
                  <a target="_blank" href="https://fanjingbo.com">@Fanjingbo</a>
                </div>
              </div>
            </div>
          </header>
    
          <div class="post-content markdown-body">
            

<p>Center Loss是2016年ECCV的一篇人脸识别<a href="https://ydwen.github.io/papers/WenECCV16.pdf">文章</a>中加入的新损失函数。原作者是使用Caffe实现的，有很多人实现了各种版本的Center Loss。但是我发现github上唯一的Pytorch版本并没有完全按照作者的方法来实现，我就打算修改一下。以下的思考都是在修改代码的过程中进行的</p>

<h2 id="一-center-loss的原理">一、Center Loss的原理</h2>

<p>要实现Center Loss，必须知道Center Loss的原理。</p>

<p>Center Loss一般是和Softmax Loss配合使用的，Softmax Loss可以使类间距离变大，而Center Loss可以使类内距离更小，下面的图片能很形象地表现出Center Loss的作用。</p>

<p><img src="/images/center_loss.jpeg" alt="center\_loss" />
Center Loss的流程大致如下：</p>

<ol>
<li>保存一个参数，这个参数存储的是feature的中心值，我们定义成centers_param</p></li>

<li><p>前传过程中计算输入的特征值features与存储的参数之间的均方误差(MSE)</p></li>

<li><p>反向传播时，feature的梯度公式如下：
<img src="/images/derivative.png" alt="derivative" />
中心值的梯度是由作者定义的，公式如下：</p></li>
</ol>

<p><img src="/images/center_loss_formula.png" alt="center\_loss\_formula" />
这样就会导致，Center Loss层输入feature的梯度很容易求，直接自动求导即可，但是，中心值的参数就需要手动更新了。</p>

<blockquote>
<p>注：作者定义的是变化而不是梯度，所以不需要乘学习率，但是需要乘以作者指定的一个系数。为了方便说明可以简化看作梯度）</p>
</blockquote>

<h2 id="二-pytorch中的backward">二、Pytorch中的backward</h2>

<p>那么到底该怎样手动更新呢？要搞清楚这点首先要了解Pytorch中的<code>backward</code>方法</p>

<p>我们看一下官方文档中<code>backward</code>的基本用法</p>

<p><img src="/images/torch_autograd_backward.png" alt="torch\_autograd\_backward" />
从文档可以看出：</p>

<ol>
<li>当<code>variables</code>是标量时，不用指定<code>grad_variables</code>（事实上，此时<code>grad_variables</code>为1），这种情况就是一般的<code>loss.backward()</code>这种用法。</p></li>
<li>当variables为非标量且<code>require_grad</code>为<code>True</code>时，需要指定<code>grad_variables</code>，文档中对<code>grad_variables</code>的解释为<code>“gradient of the differentiated function w.r.t. corresponding variables”</code>，其实意思就是损失函数相对与<code>variables</code>的梯度</li>
</ol>

<p>因此<code>backward</code>可以实现两种情况，一种是傻瓜式的，给一个<code>loss</code>，<code>backward</code>可以把所有前层<code>require_grad=True</code>的梯度算出来；而另一种是从中间层开始往前算，这种就需要知道<code>grad_variables</code>了。原理和链式法则一模一样。</p>

<p><img src="/images/back_propagation.png" alt="back\_propagation" /></p>

<h2 id="三-实现center-loss">三、实现Center Loss</h2>

<p>到现在，解决方案已经呼之欲出了：进行两次<code>backward</code>即可。如图：</p>

<p><img src="/images/backward.png" alt="backward" />
Center Loss层有两个变量，进行<code>loss.backward()</code>，两个变量都会求出对应的梯度。而<code>centers_param.backward(man_set_centers_grad)</code>，则可以直接把<code>man_set_centers_grad</code>赋值给<code>variable</code>的梯度，也就是存储中心值的梯度。那么我们连续进行以上两个<code>backward</code>，就可以实现想要的手动更新。不过需要注意的是，连续两次<code>backward</code>，会把两次梯度累加。所以在第一次<code>backward</code>后使用<code>zero_grad()</code>方法，把梯度置零即可。</p>

<blockquote>
<p>我的代码：<a href="https://github.com/flyingpot/center_loss_pytorch">https://github.com/flyingpot/center_loss_pytorch</a></p>
</blockquote>

          </div>
          
          
          <div class="post-comments">
            <div id="disqus_thread"></div>
<script>
  window.addEventListener("load", () => {
    (function() {
      
      var d = document,
        s = d.createElement("script");
      s.src = "https://fanjingbo-com.disqus.com/embed.js";
      s.setAttribute("data-timestamp", +new Date());
      (d.head || d.body).appendChild(s);
    })();
  });
</script>
<noscript
  >Please enable JavaScript to view the
  <a href="https://disqus.com/?ref_noscript"
    >comments powered by Disqus.</a
  ></noscript
>

          </div>
          
        </div>
      </div>
    </article>

    <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  var posts = document.getElementById('posts-list');
  posts && quicklink({
    el: posts,
    priority: true,
  });
</script>

    

  </body>
</html>
